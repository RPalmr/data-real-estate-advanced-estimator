{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some \"default\" libraries\n",
    "# You can now use Pandas to manipulate the Dataframe conveniently\n",
    "\n",
    "''' Data manipulation'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "''' Data visualization'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Real Estate - Advanced Estimator\n",
    "\n",
    "‚ùóÔ∏è In the previous challenge, we saw that if we have more flats than features in our dataset ($\\large n$ observations $\\large> p$ features) we can't \"solve\" the equation $\\large \\boldsymbol X \\cdot \\boldsymbol \\theta = \\boldsymbol y $. Without a deterministic formula for $\\large \\boldsymbol \\theta$, we would no longer be able to predict the prices of new flats!\n",
    "\n",
    "----\n",
    "\n",
    "üéØ In this exercise, we now have access to a bigger dataset consisting of 1000 flats, and we want to refine our prediction for the same new flat as before:\n",
    "\n",
    "- `Surface`: 3000 $ft^2$\n",
    "- `Bedrooms`: 5 \n",
    "- `Floors`: 1\n",
    "\n",
    "‚ùå Instead of solving $\\large \\boldsymbol X \\cdot \\boldsymbol \\theta = \\boldsymbol y $ with a matrix $\\large \\boldsymbol X$ of shape $ (1000,4)$ that is **`non-invertible`**...\n",
    "\n",
    "üöÄ ...we will find a $\\large {\\boldsymbol \\theta} = \\begin{bmatrix}\n",
    "     \\theta_0 \\\\\n",
    "     \\theta_1 \\\\\n",
    "    \\theta_2 \\\\\n",
    "     \\theta_3\n",
    "\\end{bmatrix}_{4 \\times 1}$ that minimizes the error $ \\large \\boldsymbol e = \\boldsymbol X \\cdot \\hat{\\boldsymbol \\theta} - \\boldsymbol y  $; this approach is called a **`Linear Regression model`**. We will measure this error $\\boldsymbol e$ using the Euclidean distance $\\large \\left\\|\\boldsymbol e\\right\\|$ and the **`Mean Squared Error.`**\n",
    "\n",
    "üëâ Let's compute $\\large \\hat{\\boldsymbol \\theta}$ to find an approximate estimation of the new flat's price.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Exploration\n",
    "\n",
    "We load the dataset `flats.csv` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flats = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/flats.csv')\n",
    "flats.head(20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ Use `scatterplots` to visually figure out <u><i>which feature gives the most information about prices:</i></u>``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "features = ['surface', 'bedrooms', 'floors']\n",
    "\n",
    "for feature in features:\n",
    "    sns.scatterplot(data=flats, x=feature, y='price')\n",
    "    plt.title(f'Scatterplot of {feature} vs. Price')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ It seems that `surface` is a stronger indicator of price than the number of bedrooms or floors. In statistics, we say that `price` is more **correlated** with `surface` than with other features. \n",
    "\n",
    "üëâ Let's double-check this by running [`pandas.DataFrame.corr`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) below, which computes correlation coefficients between each pair of columns of the DataFrame. \n",
    "\n",
    "<i> <u>Remarkable values:</u></i>\n",
    "* 1 means that the two columns are perfectly correlated üìà \n",
    "* -1 means that the two columns are perfectly inversely correlated üìâ \n",
    "* 0 means that the two columns are not *linearly* correlated üòê\n",
    "    \n",
    "<details>\n",
    "    <summary><i>Why do we use the correlation coefficient and not the covariance coefficient?</i></summary>\n",
    "\n",
    "‚úÖ <u>Similarities</u>:\n",
    "    \n",
    "- üìà Positive correlations and positive covariances between two variables X and Y mean the same thing: when X increases, Y increases, and vice-versa.\n",
    "- üìâ Negative correlations and negative covariances between two variables X and Y mean the same thing: when X increases, Y decreases, and vice-versa.\n",
    "- ü§î A null correlation and a null covariance between two variables X and Y mean the same thing: \n",
    "    - They are _not linearly correlated_ in the sense that there would exist two real numbers $a$ and $b$  such that $ Y = aX + b$ \n",
    "    - However, they could still have a different type of relation such as $Y = X^{2}$ (quadratic relation), $Y = e^{X}$ (exponential relation), $Y = ln(X)$ (logarithmic relation), $Y = sin(\\sqrt{1+X^7})$ (super weird relation), etc.\n",
    "\n",
    "‚ùóÔ∏è<u>Main differences</u>:\n",
    "\n",
    "- üòñ The covariance between two variables X and Y can be either infinitely positive or infinitely negative: $ cov(X, Y) \\in ( - \\infty ; + \\infty ) $\n",
    "    Example: if $ cov (X, Y) = 10 $ and $ cov (X, Z) = 30 $, can you say that X and Z are \"more correlated\"? _No_, because you cannot compare apples, oranges, and bananas.\n",
    "\n",
    "    üßëüèª‚Äçüè´ How to solve this problem? Consider the correlation instead, often denoted by the Greek Letter $\\rho$ (pronounce \"rh√¥\")\n",
    "\n",
    "üëâ <u>Consequence</u>:\n",
    "\n",
    "- You can view the correlation as a _standardized covariance_, we simply divide the covariance by both the standard deviation of $X$ and the standard deviation of $Y$    \n",
    "$$ \\large  \\rho(X,Y) = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y} \\in [0;1]$$\n",
    "\n",
    "Continuing our example: suppose now that $ \\rho(X, Y) = 0.80 $ and $ \\rho(X, Z) = 0.15 $, would you still want to say that X and Z are more correlated? No; in fact, X and Y are more correlated than X and Z!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "correlation_matrix = flats.corr()\n",
    "\n",
    "surface_price_correlation = correlation_matrix['surface']['price']\n",
    "print(f\"Correlation between 'surface' and 'price': {surface_price_correlation}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé® For a quicker glimpse of this matrix, you can use a **heatmap** from [`seaborn.heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
    "\n",
    "<details>\n",
    "    <summary><i>Additional tips to display a nicer correlation matrix</i></summary>\n",
    "\n",
    "- `cmap`: **Seaborn** being a visualization library built on top of **Matplotlib**, you can use the argument [`cmap`](https://matplotlib.org/stable/tutorials/colors/colormaps.html), which stands for _color map_\n",
    "- `annot`: to read the correlations even faster, you can show the correlation coefficients directly on the colored heatmap\n",
    "- `annot_kws`: you can customize how the correlation coefficients appear\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™ Test your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'flats',\n",
    "    shape=flats.shape,\n",
    "    columns=flats.columns\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Estimator with 1 Feature\n",
    "\n",
    "Let's try to build a statistical estimator of **price** as a function of only one feature: the **surface**.\n",
    "\n",
    "üéØ Let's try to fit a **linear regression** between the two variables.\n",
    "\n",
    "Practically speaking, we want to choose the best parameters $\\hat{\\boldsymbol \\theta}$ = (`slope`, `intercept`) such that the `predicted price = slope * surface + intercept` is as close as possible to the `price` in terms of Mean Squared Error (MSE).\n",
    "\n",
    "üìÖ During the next weeks, we will discover and study different models (Linear Regression, KNN, Logistic Regression, Neural Networks, etc.). \n",
    "\n",
    "üëâ An important part of your job is to choose the right model and optimize the parameters to make the best predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Visual Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìImplement the function `plot_line(slope, intercept)`\n",
    "\n",
    "When given the `slope` and `intercept` arguments, this function creates an array `predicted_price` and plots that array on top of the original (scattered) data.\n",
    "\n",
    "When you are done with coding the function, play with different values of `(slope, intercept)` until you find a ‚Äúgood linear approximation‚Äù of the data. Can you find the best fit?\n",
    "\n",
    "Visual guideline:\n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/03-Maths/01-Algebra-Calculus/line_of_best_fit.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line(data, slope, intercept):\n",
    "    x = data['surface'].values\n",
    "\n",
    "    y_pred = slope * x + intercept\n",
    "\n",
    "    sns.scatterplot(x=\"surface\", y=\"price\", data=data, label='Original Data', color='blue', marker='o')\n",
    "\n",
    "    plt.plot(x, y_pred, label='Predicted Line', color='red')\n",
    "\n",
    "    plt.xlabel('Surface')\n",
    "    plt.ylabel('Price')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "x = flats['surface'].values\n",
    "y = flats['price'].values\n",
    "\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "\n",
    "m = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)\n",
    "b = y_mean - m * x_mean\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by running the cell below, then change the values to see if you can approximate a good line of best fit!\n",
    "\n",
    "*Hint: you can run `plot_line()` multiple times in the same cell to get several lines on the same scatterplot* üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"surface\", y=\"price\", data=flats)\n",
    "plot_line(flats, m, b)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòÖ Not so easy (and not very ‚Äúscientific‚Äù), right?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Computational Approach\n",
    "\n",
    "üî• To make sure that our estimator line is the best possible one, we need to compute the **Mean Squared Error** between the **real prices** and the **predicted prices**!\n",
    "\n",
    "üëâ Remember that:\n",
    "- For each apartment, `predicted_price = slope * surface + intercept`\n",
    "- Both the **vector of real prices** and the **vector of predicted prices** are of shape $ (1000, 1)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.1) Squared Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Step 1</b></u>\n",
    "\n",
    "‚ùìFor each row (_i.e. flat_), we should evaluate the `squared_error = (price - predicted_price)**2`‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_squared_errors(slope, intercept, surfaces, prices):\n",
    "    \"\"\"\n",
    "    TODO: return an array containing the squared errors between\n",
    "    all real prices from the dataset and the predicted prices\n",
    "    \"\"\"\n",
    "    squared_errors = []\n",
    "\n",
    "    for surface, price in zip(surfaces, prices):\n",
    "        predicted_price = slope * surface + intercept\n",
    "\n",
    "        squared_error = (price - predicted_price) ** 2\n",
    "\n",
    "        squared_errors.append(squared_error)\n",
    "\n",
    "    squared_errors = np.array(squared_errors)\n",
    "\n",
    "    return squared_errors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° A general principle in Data Science/Modeling is that $ \\large error = f(\\boldsymbol y, \\hat{\\boldsymbol y})$ where:\n",
    "\n",
    "- $ \\large \\boldsymbol y $ is the real value\n",
    "- $ \\large \\hat{\\boldsymbol y} $ the predicted valute\n",
    "- $ \\large f$ is often called a **Loss Function** or a **Cost Function** \n",
    "    - üìÜ See `Machine Learning I > Model Tuning`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.2) Mean Squared Errors (MSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Step 2</b></u>\n",
    "\n",
    "‚ùì Create the `mse` function which should return the mean of the array returned by the `squared_errors` function. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(slope, intercept, surfaces, prices):\n",
    "    '''TODO: Return the mean of the array contained in squared_errors as a float.'''\n",
    "    squared_errors = compute_squared_errors(slope, intercept, surfaces, prices)\n",
    "\n",
    "    mse = squared_errors.mean()\n",
    "\n",
    "    return mse\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ In section _(2.1) Visual approach_, you visually tried to estimate the \"best line\", which consists in finding the best pair `(slope, intercept)`. \n",
    "\n",
    "‚ùìUsing this \"best pair\", compute the MSE of your estimator. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "best_slope = 0.1\n",
    "best_intercept = 5.0\n",
    "\n",
    "best_mse = compute_mse(best_slope, best_intercept, surfaces, prices)\n",
    "\n",
    "print(\"Mean Squared Error with Best Pair (slope, intercept):\", best_mse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Finding the Best Parameters\n",
    "\n",
    "üëâ Keep playing with different values for `slope` and `intercept` and try to get the best fit by hand!  Notice how hard it is to optimize both parameters at the same time.\n",
    "\n",
    "üëá Follow the steps below to get an idea of one potential approach:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.1) Finding the Best Slope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i><u> Step 1 :</u></i></b>\n",
    "\n",
    "Start by fixing an  `initial_intercept` with your best estimate, then find a slope that approximately minimizes the function  `mse = f(slope)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_intercept = 5.0\n",
    "\n",
    "slope_values = np.arange(0.1, 5.0, 0.1)\n",
    "\n",
    "best_slope = None\n",
    "best_mse = float('inf')\n",
    "\n",
    "for slope in slope_values:\n",
    "    mse = compute_mse(slope, initial_intercept, surfaces, prices)\n",
    "\n",
    "    if mse < best_mse:\n",
    "        best_slope = slope\n",
    "        best_mse = mse\n",
    "\n",
    "print(\"Best Slope:\", best_slope)\n",
    "print(\"Corresponding MSE:\", best_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_slope = 2.5\n",
    "\n",
    "num_slopes = 100\n",
    "min_slope = 0.1\n",
    "max_slope = 0.4\n",
    "\n",
    "slopes = np.linspace(min_slope, max_slope, num_slopes)\n",
    "\n",
    "slopes = np.linspace(min_slope, max_slope, num_slopes)\n",
    "\n",
    "print(\"Slope Values:\")\n",
    "print(slopes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = np.linspace(0.1, 0.4, 100)\n",
    "\n",
    "mse_list = []\n",
    "\n",
    "for slope in slopes:\n",
    "    mse = compute_mse(slope, initial_intercept, surfaces, prices)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "print(\"MSEs for each slope value:\")\n",
    "print(mse_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìPlot MSEs vs. slopes. Do you see a minimum‚ùì\n",
    "\n",
    "üôÉ If not, try another range of slopes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCompute the minimum value of MSE for your `initial_intercept`, and the corresponding `slope_best` value‚ùì\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Hint</i></summary>\n",
    "    \n",
    "Here you can use Python's built-in `.min()` function, as well as the `List.index()` method\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "min_mse = min(mse_list)\n",
    "slope_best = slopes[mse_list.index(min_mse)]\n",
    "\n",
    "print(\"Minimum MSE:\", min_mse)\n",
    "print(\"Corresponding Best Slope:\", slope_best)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.3.2) Finding the Best Intercept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i><u> Step 2 :</u></i></b>\n",
    "\n",
    "üî® Now, let's fix the slope to that `slope_best` value, then re-use the previous approach to find \"the\" `intercept_best`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_intercept = 20.0\n",
    "intercept_range = 10\n",
    "\n",
    "min_intercept = optimal_intercept - (intercept_range / 2)\n",
    "max_intercept = optimal_intercept + (intercept_range / 2)\n",
    "\n",
    "num_intercepts = 100\n",
    "intercepts = np.linspace(min_intercept, max_intercept, num_intercepts)\n",
    "\n",
    "print(\"Intercept Values:\")\n",
    "print(intercepts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts = np.linspace(optimal_intercept - 5, optimal_intercept + 5, 100)\n",
    "\n",
    "mse_list = []\n",
    "\n",
    "for intercept in intercepts:\n",
    "    mse = compute_mse(slope_best, intercept, surfaces, prices)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "print(\"MSEs for each intercept value:\")\n",
    "print(mse_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(slopes, mse_list, marker='o', linestyle='-')\n",
    "plt.xlabel('Slope Values')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE vs. Slope Values')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCompute `mse_min`, the minimum value of MSEs when slope is equal to `slope_best`, and store the corresponding best intercept as `intercept_best`‚ùì\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_min = None\n",
    "intercept_best = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "mse_min = min(mse_list)\n",
    "intercept_best = intercepts[mse_list.index(mse_min)]\n",
    "\n",
    "print(\"Minimum MSE with Best Slope (slope_best):\", mse_min)\n",
    "print(\"Corresponding Best Intercept (intercept_best):\", intercept_best)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üß™ Test your code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'univariate',\n",
    "    mse_min=mse_min,\n",
    "    slope_best=slope_best,\n",
    "    intercept_best=intercept_best\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ **Great job**! By adding a non-zero intercept parameter, we were able to reduce the MSE even more (feel free to plot the regression line on your scatter plot to visually confirm the approximate fit).\n",
    "\n",
    "‚ùìHowever, what guarantees that these (`intercept_best`, `slope_best`) parameters are really the best ones? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üëÄ Explanations</summary>\n",
    "\n",
    "We could maybe find an even better slope value by repeating step ‚ë†, this time fixing the intercept at `intercept_best`, and then repeating step ‚ë° with the new slope to adjust the intercept again.\n",
    "\n",
    "To find the global minimum of a 2-parameter function `rmse = f(slope, intercept)`, we may have to repeat steps ‚ë† and ‚ë° indefinitely until values converge towards absolute minima - with no guarantee of success.\n",
    "\n",
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/decision-science/real-estate-minimizer.png'>\n",
    "\n",
    "üí™ You've just discovered one of the most fundamental aspects of Machine Learning: **the iterative process of finding minima**.\n",
    "\n",
    "üëâ As you might guess, in the world of Data Science, algorithms have been developed to automate and optimize such processes. In the next few weeks, you will discover the power of other algorithms such as **Gradient Descent**, and Python libraries such as `Statsmodels` that perform this iterative process for you.\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Run the cells below if you are curious and want to find the real best slope and intercept for this dataset, computed using **Gradient Descent** (üìÜ covered in **Machine Learning I > Under The Hood**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out (Seaborn visual solution)\n",
    "sns.regplot(data=flats, x='surface', y='price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this out (statsmodels, exact solution)!\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "regression = smf.ols(formula= 'price ~ surface', data=flats).fit()\n",
    "\n",
    "print(\"intercept_best\", regression.params['Intercept'])\n",
    "print(\"slope_best\", regression.params['surface'])\n",
    "print('mse_best: ', np.mean(regression.resid**2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìLet's go back to our initial question: what is your new prediction for the 5th flat below? How does it compare with your initial prediction based on only 4 flats?\n",
    "\n",
    "- `surface`: 3000 $ft^2$\n",
    "- `bedrooms`: 5 \n",
    "- `floors`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for the 5th flat: 2776215.485446571\n"
     ]
    }
   ],
   "source": [
    "# compute predicted price (Remember that the real price is 750,000$)\n",
    "X = [[1, flat['surface'], flat['bedrooms'], flat['floors']] for flat in data]\n",
    "y = [flat['price'] for flat in data]\n",
    "\n",
    "XtX = [[sum(X[i][j] * X[k][j] for k in range(len(X))) for i in range(4)] for j in range(4)]\n",
    "Xty = [sum(X[i][j] * y[i] for i in range(4)) for j in range(4)]\n",
    "\n",
    "intercept, slope_surface, slope_bedrooms, slope_floors = [Xty[j] / XtX[j][j] for j in range(4)]\n",
    "\n",
    "new_flat_features = [1, 3000, 5, 1]\n",
    "predicted_price = sum(new_flat_features[i] * [intercept, slope_surface, slope_bedrooms, slope_floors][i] for i in range(4))\n",
    "\n",
    "print(\"Predicted price for the 5th flat:\", predicted_price)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è It's better than our initial deterministic estimator based on only 4 flats, but obviously we are missing the information provided by the number of bedrooms and floors in this prediction!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Estimator with _All_ Features (`surface`, `bedrooms`, `floors`)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° A linear regression with three features (**Multivariate Linear Regression**) works the same way as with one feature, but instead of determining only 2 parameters to minimize the RMSE (`intercept` and `slope`), we will need to find 4 parameters: $\\hat{\\boldsymbol \\theta}$ = (`intercept`, `slope_surface`, `slope_bedrooms`, `slope_floors`).\n",
    "\n",
    "üóì There will be a lecture fully dedicated to **Multivariate Linear Regression** in the **Decision Science** module.\n",
    "\n",
    "üóì The same **Gradient Descent** iterative method is applicable, and you will code it yourself by hand later in the bootcamp.\n",
    "\n",
    "üëâ Meanwhile, feel free to run the cell below to see the final result of this multivariate regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    18.154854\n",
       "surface       0.286953\n",
       "bedrooms    -21.623564\n",
       "floors       -3.811868\n",
       "dtype: float64"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the four regression coefficients by running this cell\n",
    "regression = smf.ols(formula= 'price ~ surface + bedrooms + floors', data=flats).fit()\n",
    "regression.params\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Now that we have found the best parameters $\\boldsymbol {\\hat \\theta}= \\begin{bmatrix}\n",
    "     \\theta_0 \\\\\n",
    "     \\theta_1 \\\\\n",
    "    \\theta_2 \\\\\n",
    "     \\theta_3 \\\\\n",
    "\\end{bmatrix}_{4 \\times 1} = \n",
    "\\begin{bmatrix}\n",
    "     \\theta_{intercept} \\\\\n",
    "     \\theta_{surface}\\\\\n",
    "    \\theta_{bedrooms} \\\\\n",
    "     \\theta_{floors}\n",
    "\\end{bmatrix}_{4 \\times 1} = \n",
    "\\begin{bmatrix}\n",
    "    18.154854 \\\\\n",
    "    0.286953 \\\\\n",
    "    -21.623564 \\\\\n",
    "    -3.811868\n",
    "\\end{bmatrix}_{4 \\times 1}\n",
    "$, \n",
    "\n",
    "we can predict the price of the new flat with:\n",
    "* $3000 ft^2$\n",
    "* $5$ bedrooms\n",
    "* located on the $1st$ floor\n",
    "\n",
    "$$ \\hat{y_5} = \\theta_0 + \\theta_1 \\times 3000 + \\theta_2 \\times 5 + \\theta_3 \\times 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price for the 5th flat: 767.0851073102864\n"
     ]
    }
   ],
   "source": [
    "# Compute the newly predicted price for the 5th flat? Is this prediction better?\n",
    "intercept, coef_surface, coef_bedrooms, coef_floors = regression.params\n",
    "\n",
    "surface = 3000\n",
    "bedrooms = 5\n",
    "floors = 1\n",
    "\n",
    "predicted_price = intercept + coef_surface * surface + coef_bedrooms * bedrooms + coef_floors * floors\n",
    "\n",
    "print(\"Predicted price for the 5th flat:\", predicted_price)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Concluding Remarks on Linear Algebra üß†"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **optimization problem** can be summarized as follows\n",
    "\n",
    "- We need to find a vector of parameters $\\hat{\\boldsymbol \\theta} = \\begin{bmatrix}\n",
    "     \\theta_{intercept} \\\\\n",
    "     \\theta_{surface}\\\\\n",
    "    \\theta_{bedrooms} \\\\\n",
    "     \\theta_{floors}\n",
    "\\end{bmatrix}_{4 \\times 1}$ \n",
    "\n",
    "- That minimizes an error $e = \\left\\|\\boldsymbol X\\cdot \\hat{\\boldsymbol \\theta} - \\boldsymbol y  \\right\\|^2$\n",
    "\n",
    "- For a given matrix of features $\\boldsymbol X$ [constant, surfaces, floors, bedrooms]  $\\begin{bmatrix}\n",
    "    1 & 620 & 1 & 1 \\\\\n",
    "    1 & 3280 & 4 & 2 \\\\\n",
    "    ... \\\\\n",
    "    1 & 1900 & 2 & 2 \\\\\n",
    "    1 & 1320 & 3 & 3\n",
    "   \\end{bmatrix}_{n \\times 4}\n",
    "$\n",
    "\n",
    "- and a vector of observations $\\boldsymbol y  = \\begin{bmatrix}\n",
    "           y_{1} \\\\\n",
    "           y_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           y_{1000}\n",
    "         \\end{bmatrix}$ (prices)\n",
    "\n",
    "Such a $\\large \\hat{\\theta}$ is reached when the \"derivatives\" of $\\boldsymbol e$, that is $ \\large 2 \\boldsymbol X^T\\cdot (\\boldsymbol X \\cdot \\hat{\\theta}‚àí\\boldsymbol y )$, equal zero (üëâ proof during the Decision Science module). \n",
    "    \n",
    "In other words, we need to solve the linear system $\\large (\\boldsymbol X^T\\cdot \\boldsymbol X)\\cdot \\hat{\\boldsymbol \\theta}=\\boldsymbol X^T \\cdot \\boldsymbol y $. \n",
    "    \n",
    "This linear system has a unique solution provided that no column of $\\boldsymbol X$ can be expressed as a linear combination of the others: in that case, $ \\large (\\boldsymbol X^T \\cdot \\boldsymbol X)^{-1}$ is invertible and the minimum is reached when $\\large \\hat{\\boldsymbol \\theta} = (\\boldsymbol X^T\\cdot \\boldsymbol X)^{-1} \\cdot \\boldsymbol X^T \\cdot \\boldsymbol y $. Notice how $\\boldsymbol X$ does not need to be squared anymore compared to the first challenge üí™.\n",
    "\n",
    "üí• However, keep in mind that **inverting matrices is computationally complex**. That is why other methods have been developed to find the minimum of a function, such as **Gradient Descent**.\n",
    "    \n",
    "üìö Read more on [Stats.StackExchange](https://stats.stackexchange.com/a/278779) if you are interested!\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! \n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
